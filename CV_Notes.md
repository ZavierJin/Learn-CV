# 计算机视觉

## Reference

https://www.bilibili.com/video/BV1nJ411z7fe?spm_id_from=333.999.0.0

P8

## 引入

### 基础

- 超参数
  - 需要提前给定的参数，算法的设定，不一定可以在训练中学习，
- 数据集分割
  - 大数据集：分割为三部分，训练集、验证集（Validation）、测试集。调整超参在验证集上最佳，测试集只在最后使用
  - 小数据集：K折交叉验证，分割测试集和训练集，再在训练集中使用K折验证

### KNN

- 最邻近分类：
  - 训练：记录所有数据和标签
  - 预测：最相似的图片
  - 距离函数：
    - L1曼哈顿距离：图片像素差的绝对值的累加，越小差异越小。坐标轴旋转会变化，如果输入向量的每个元素都有特殊含义，用L1
    - L2欧式距离：坐标轴旋转不会变化（圆形），如果输入向量比较普通，用L2
- KNN
  - 找到K个相邻的进行投票
  - 对噪声更鲁棒，对未训练过的测试集更好，泛化性提升
- 特点
  - 适用范围比较广，只要定义距离函数就可以用
  - 好的方法应该训练耗时较长，预测耗时较短，这个正好相反
  - 距离函数对于图像视觉感知很难衡量
  - 维数危机，运算复杂度指数级

### 线性分类

-  $f(x,W) = Wx + b$，多样本输入$f(X,W) = WX + b$
-  系数矩阵W: $10*3072$ (每一行代表某一类别的模板)，输入向量x: $32*32*3=3072$，偏置b: $10*1$
-  几何解释：高维空间线性分割
- 问题：非线性情况难以分割 

## 损失函数

- 用于衡量W的好坏。其形式的选择与你关系怎么样的错误有关，如果对于大错误需要放大，则用平方。

### SVM

- SVM(支持向量机)，是一种解释性强且效果好的线性分类器。
- 使用多分类SVM作为损失函数，单个样本的损失函数$$\displaystyle L_{i}=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+\Delta\right)=\sum_{j \neq y_{i}} \max \left(0, w_j^Tx_i-w_{y_i}^Tx_i+\Delta\right) $$，其中输入$x_i$，标签$y_i$，模型输出的score是K维度的$s$，$s$的每一个元素为$s_j$，而$s$中真实标签的元素为$s_{y_i}$。$s_{j}-s_{y_{i}}$即计算其他类对应的元素与真实标签对应元素的score差值，如果超出一定阈值($\Delta$，一般选择1)，就需要累加到损失函数上。因此，我们总是希望正确标签的score大而错误标签的score比较小。
- 如果$s$的每个元素都接近于0，则总平均损失为K-1。
- 未防止过拟合，在平均后添加正则项(这里选用L2范数)，总损失函数为$$\displaystyle L=\frac 1N\sum_{i=1}^NL_{i}+\lambda ||W||^2=\sum_{i=1}^N\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+\Delta\right)+\lambda ||W||_{2}$$。根据奥卡姆剃刀原则，选择简单的W，超参数$\lambda$。

### Softmax

- 多项逻辑斯帝损失，与概率有关，使正确分类的概率趋向于1。
- 单个样本的损失函数$\displaystyle L_i=-log(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}})$。另一种计算方法是对$s$先取指数再归1化，然后$y_i$位置的值就是$L_i$。

## 误差传递

### 梯度计算

-  计算图，每层间链式传递$$\displaystyle \frac{\partial L}{\partial x}=\frac{\partial f}{\partial x}\frac{\partial L}{\partial f} $$.
- 如果输入的量x是向量，单层的偏导$$\displaystyle \frac{\partial f}{\partial x}$$可以整合为Jacobian矩阵$$\displaystyle [\frac{\partial f_j}{\partial x_i}]$$，因为模块是多入多出的。
- 如果链式求导的中间变量是向量，需要对每个偏导累加。
- 梯度的维数与向量的维数相同
- 例：$$\displaystyle 1_{k=i}=\left\{\begin{matrix} 1, k=i\\0, k!=i\end{matrix}\right.$$。

![Screenshot 2022-05-19 163511](D:\zju\study\08_大四下\课外阅读\Learn-CV\Image\Screenshot 2022-05-19 163511.png)

### 优化方法

- 梯度下降法：计算梯度（解析法和数值法，数值法在变量多时太耗时），步长*梯度来下降。步长也称学习率。
- 其他方法
  - 带动量的梯度下降
  - Adam优化
- 当数据集很大时，即使用解析法计算梯度也很慢，因此可以采样随机梯度下降法(SGD)。不计算整个数据集的损失和梯度，而是在每次迭代中选取部分训练集(MiniBatch)，估计误差总和和梯度。

## 神经网络

- 全连接网络：输入层、隐藏层、输出层
- 激活函数：非线性，softmax、ReLU
- 算法实现：每个神经元的输入先与权重相乘，再累加，可以转化为矩阵运算。
- 2012年，第一个攻克的领域是语音识别，同年卷积神经网络让图像识别领域也开始使用
  - Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition
  - Imagenet classification  with deep convolutional neural networks

## 卷积神经网络

### 卷积层

- 卷积核Filter与输入图像深度相同，$w^Tx+b$小区域分别展成1维后点积再加偏置。严格上说是滤波，并不是数字图像处理领域的卷积，那边的要旋转180度。
- **维数计算**：NxNxD的输入图用FxFxD的卷积核，采用S的步长，得到的输出边长为$O=(N-F)/S+1$，输出OxOx1的结果
- **边缘填充**：为了保持输出和输入的边长相同。否则在深度网络中边长会迅速变小，可用信息量会变少。单边填充P，则输出的边长计算公式变化为：$O=(N+2*P-F)/S+1$.
- **每层参数**：$(F*F*D+1)*num$，其中num是卷积核个数，+1是因为有偏置项
- **输出的深度**是由卷积核个数决定的，一般是2的次方。如果有多个核，可以分别对输入进行卷积，每次得到一张激活映射（activation map），最后对多个结果沿深度方向组合。
- 特征图展示了当输入是什么状态时使激活函数输出最大
- 感受视野（reception field），FxF
- 卷积核大小、步长、个数等都是超参数

### 池化层

- 平面上的降采样，深度方向不变。使生成的结果更小更容易控制
- 步长一般使移动过程不重叠。池化一般不需要使用边缘填充。
- 常见方法：最大值池化，因为激活映射得到的是卷积核在空间位置的受激程度。在类似识别和检测任务中，使用最大值来激活比较好。
- 常见大小：2*2

### 全连接层

- 将输出图像拉成1维，与普通的神经网络相连。
- 最后的输出图像每个元素描述的是复杂的特征，是前面一系列复杂的受激活情况

