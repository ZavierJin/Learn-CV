# 计算机视觉

P7

## KNN

- 最邻近分类：
  - 训练：记录所有数据和标签
  - 预测：最相似的图片
  - 距离函数：
    - L1曼哈顿距离：图片像素差的绝对值的累加，越小差异越小。坐标轴旋转会变化，如果输入向量的每个元素都有特殊含义，用L1
    - L2欧式距离：坐标轴旋转不会变化（圆形），如果输入向量比较普通，用L2
- KNN
  - 找到K个相邻的进行投票
  - 对噪声更鲁棒，对未训练过的测试集更好，泛化性提升
- 超参数
  - 需要提前给定的参数，算法的设定，不一定可以在训练中学习，
- 数据集分割
  - 大数据集：分割为三部分，训练集、验证集（Validation）、测试集。调整超参在验证集上最佳，测试集只在最后使用
  - 小数据集：K折交叉验证，分割测试集和训练集，再在训练集中使用K折验证
- 特点
  - 适用范围比较广，只要定义距离函数就可以用
  - 好的方法应该训练耗时较长，预测耗时较短，这个正好相反
  - 距离函数对于图像视觉感知很难衡量
  - 维数危机，运算复杂度指数级

## 线性分类

-  $f(x,W) = Wx + b$，多样本输入$f(X,W) = WX + b$
-  系数矩阵W: $10*3072$ (每一行代表某一类别的模板)，输入向量x: $32*32*3=3072$，偏置b: $10*1$
-  几何解释：高维空间线性分割
- 问题：非线性情况难以分割 

## 损失函数

- 用于衡量W的好坏。其形式的选择与你关系怎么样的错误有关，如果对于大错误需要放大，则用平方。

### SVM

- SVM(支持向量机)，是一种解释性强且效果好的线性分类器。
- 使用多分类SVM作为损失函数，单个样本的损失函数$$\displaystyle L_{i}=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+\Delta\right)=\sum_{j \neq y_{i}} \max \left(0, w_j^Tx_i-w_{y_i}^Tx_i+\Delta\right) $$，其中输入$x_i$，标签$y_i$，模型输出的score是K维度的$s$，$s$的每一个元素为$s_j$，而$s$中真实标签的元素为$s_{y_i}$。$s_{j}-s_{y_{i}}$即计算其他类对应的元素与真实标签对应元素的score差值，如果超出一定阈值($\Delta$，一般选择1)，就需要累加到损失函数上。因此，我们总是希望正确标签的score大而错误标签的score比较小。
- 如果$s$的每个元素都接近于0，则总平均损失为K-1。
- 未防止过拟合，在平均后添加正则项(这里选用L2范数)，总损失函数为$$\displaystyle L=\frac 1N\sum_{i=1}^NL_{i}+\lambda ||W||^2=\sum_{i=1}^N\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+\Delta\right)+\lambda ||W||_{2}$$。根据奥卡姆剃刀原则，选择简单的W，超参数$\lambda$。

### Softmax

- 多项逻辑斯帝损失，与概率有关，使正确分类的概率趋向于1。
- 单个样本的损失函数$\displaystyle L_i=-log(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}})$。另一种计算方法是对$s$先取指数再归1化，然后$y_i$位置的值就是$L_i$。

## 优化方法

- 梯度下降法：计算梯度（解析法和数值法，数值法在变量多时太耗时），步长*梯度来下降。步长也称学习率。
- 其他方法
  - 带动量的梯度下降
  - Adam优化
- 当数据集很大时，即使用解析法计算梯度也很慢，因此可以采样随机梯度下降法(SGD)。不计算整个数据集的损失和梯度，而是在每次迭代中选取部分训练集(MiniBatch)，估计误差总和和梯度。
